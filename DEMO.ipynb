{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date3</th>\n",
       "      <th>Departures</th>\n",
       "      <th>Class</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rating_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-01 00:00:00</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Economy</td>\n",
       "      <td>Recently I travelled for business from Cape To...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-01 00:00:00</td>\n",
       "      <td>International</td>\n",
       "      <td>Economy</td>\n",
       "      <td>I want to thanks to Jane for her excellent ser...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-01 00:00:00</td>\n",
       "      <td>International</td>\n",
       "      <td>Economy</td>\n",
       "      <td>not a bad airline to fly with could do with a ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-01 00:00:00</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Business Class</td>\n",
       "      <td>Dear Kenya Airways ,\\r\\n\\r\\nI am on my way to ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-01 00:00:00</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Economy</td>\n",
       "      <td>Absolutely appalling airline, never ever use i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>26-01-2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better than I expected Much better service tha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>30-08-2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Request for immediate action. I have never had...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>02-07-2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Never again Flew with Kenya airways last year ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>21-11-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEVER AGAIN - RUBISH COMPANY !!! We travelled ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3064</th>\n",
       "      <td>02-07-2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worse Customer service ever!!! Will never ever...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3065 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Date3     Departures           Class  \\\n",
       "0     2022-09-01 00:00:00         Africa         Economy   \n",
       "1     2022-11-01 00:00:00  International         Economy   \n",
       "2     2022-05-01 00:00:00  International         Economy   \n",
       "3     2022-11-01 00:00:00         Africa  Business Class   \n",
       "4     2022-11-01 00:00:00         Africa         Economy   \n",
       "...                   ...            ...             ...   \n",
       "3060           26-01-2017            NaN             NaN   \n",
       "3061           30-08-2016            NaN             NaN   \n",
       "3062           02-07-2016            NaN             NaN   \n",
       "3063           21-11-2015            NaN             NaN   \n",
       "3064           02-07-2014            NaN             NaN   \n",
       "\n",
       "                                                 Review  Rating  \\\n",
       "0     Recently I travelled for business from Cape To...     4.0   \n",
       "1     I want to thanks to Jane for her excellent ser...     5.0   \n",
       "2     not a bad airline to fly with could do with a ...     4.0   \n",
       "3     Dear Kenya Airways ,\\r\\n\\r\\nI am on my way to ...     1.0   \n",
       "4     Absolutely appalling airline, never ever use i...     1.0   \n",
       "...                                                 ...     ...   \n",
       "3060  Better than I expected Much better service tha...     0.0   \n",
       "3061  Request for immediate action. I have never had...     0.0   \n",
       "3062  Never again Flew with Kenya airways last year ...     0.0   \n",
       "3063  NEVER AGAIN - RUBISH COMPANY !!! We travelled ...     1.0   \n",
       "3064  Worse Customer service ever!!! Will never ever...     0.0   \n",
       "\n",
       "     Rating_Description  \n",
       "0                  Good  \n",
       "1             Excellent  \n",
       "2                  Good  \n",
       "3                  Poor  \n",
       "4                  Poor  \n",
       "...                 ...  \n",
       "3060                NaN  \n",
       "3061                NaN  \n",
       "3062                NaN  \n",
       "3063                NaN  \n",
       "3064                NaN  \n",
       "\n",
       "[3065 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv('Refined data/KQ_Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2024-08-16 15:53:28.839 No runtime found, using MemoryCacheStorageManager\n",
      "2024-08-16 15:53:28.859 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# Initialize NLTK tools\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [ps.stem(w) for w in words if w not in stop_words and w.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_review_sentiment(review):\n",
    "    sentiment_score = sia.polarity_scores(review)['compound']\n",
    "    if sentiment_score > 0.1:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def determine_final_sentiment(row):\n",
    "    review_sentiment = get_review_sentiment(row['Review'])\n",
    "    rating = row['Rating']\n",
    "    \n",
    "    if review_sentiment == 'positive' or rating >= 4:\n",
    "        return 'positive'\n",
    "    elif review_sentiment == 'negative' or rating <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Use Streamlit's new caching methods\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    # Replace 'your_file.csv' with the path to your dataset\n",
    "    df = pd.read_csv('Refined data/KQ_Data.csv')\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Apply the function to determine sentiment\n",
    "df['sentiment'] = df.apply(determine_final_sentiment, axis=1)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_reviews = df['Review']\n",
    "X_ratings = df['Rating']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Apply the preprocessing to the reviews\n",
    "X_reviews = X_reviews.apply(preprocess_text)\n",
    "\n",
    "# Initialize the vectorizer and fit/transform the text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_reviews_transformed = vectorizer.fit_transform(X_reviews).toarray()\n",
    "\n",
    "# Convert ratings into a 2D array with one feature (reshape for concatenation)\n",
    "X_ratings_transformed = X_ratings.values.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the review features with the rating feature\n",
    "X_combined = np.hstack((X_reviews_transformed, X_ratings_transformed))\n",
    "\n",
    "# Handle missing values by imputing (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_combined = imputer.fit_transform(X_combined)\n",
    "\n",
    "# Encode the target variable (sentiment)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Initialize SMOTE and apply it to the data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "st.write(classification_report(y_test, y_pred))\n",
    "st.write('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('logistic_model.pkl', 'wb') as file:\n",
    "    pickle.dump(logistic_model, file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "st.write(\"Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       226\n",
      "           1       0.95      0.95      0.95       218\n",
      "           2       0.94      0.96      0.95       224\n",
      "           3       0.99      0.90      0.94       236\n",
      "\n",
      "    accuracy                           0.94       904\n",
      "   macro avg       0.95      0.95      0.94       904\n",
      "weighted avg       0.95      0.94      0.94       904\n",
      "\n",
      "Accuracy: 0.9446902654867256\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "data = df\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_reviews = data['Review']\n",
    "X_ratings = data['Rating']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the stemmer and stopwords\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and perform stemming\n",
    "    words = [ps.stem(w) for w in words if w not in stop_words and w.isalpha()]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing to the reviews\n",
    "X_reviews = X_reviews.apply(preprocess_text)\n",
    "\n",
    "# Initialize the vectorizer and fit/transform the text data\n",
    "vectorizerr= TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "X_reviews_transformed = vectorizerr.fit_transform(X_reviews).toarray()\n",
    "\n",
    "# Convert ratings into a 2D array with one feature (reshape for concatenation)\n",
    "X_ratings_transformed = X_ratings.values.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the review features with the rating feature\n",
    "X_combined = np.hstack((X_reviews_transformed, X_ratings_transformed))\n",
    "\n",
    "# Handle missing values by imputing (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_combined = imputer.fit_transform(X_combined)\n",
    "\n",
    "# Encode the target variable (sentiment)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Initialize SMOTE and apply it to the data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Random Forest model\n",
    "best_rff= RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "best_rff.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = best_rff.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('best_rff.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rff, file)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open('vectorizerr.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizerr, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.92      0.85      0.89       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9211242067089755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Initialize NLTK tools\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [ps.stem(w) for w in words if w not in stop_words and w.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_review_sentiment(review):\n",
    "    sentiment_score = sia.polarity_scores(review)['compound']\n",
    "    if sentiment_score > 0.1:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def determine_final_sentiment(row):\n",
    "    review_sentiment = get_review_sentiment(row['Review'])\n",
    "    rating = row['Rating']\n",
    "    \n",
    "    if review_sentiment == 'positive' or rating >= 4:\n",
    "        return 'positive'\n",
    "    elif review_sentiment == 'negative' or rating <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "# Apply the function to determine sentiment\n",
    "df['sentiment'] = df.apply(determine_final_sentiment, axis=1)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_reviews = df['Review']\n",
    "X_ratings = df['Rating']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Apply the preprocessing to the reviews\n",
    "X_reviews = X_reviews.apply(preprocess_text)\n",
    "\n",
    "# Initialize the vectorizer and fit/transform the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_reviews_transformed = vectorizer.fit_transform(X_reviews).toarray()\n",
    "\n",
    "# Convert ratings into a 2D array with one feature (reshape for concatenation)\n",
    "X_ratings_transformed = X_ratings.values.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the review features with the rating feature\n",
    "X_combined = np.hstack((X_reviews_transformed, X_ratings_transformed))\n",
    "\n",
    "# Handle missing values by imputing (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_combined = imputer.fit_transform(X_combined)\n",
    "\n",
    "# Encode the target variable (sentiment)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Initialize SMOTE and apply it to the data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer saved to tfidf_vectorizer.pkl\n",
      "Model saved to logistic_regression_model.pkl\n",
      "Label encoder saved to label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the vectorizer\n",
    "vectorizer_filename = 'tfidf_vectorizer.pkl'\n",
    "joblib.dump(vectorizer, vectorizer_filename)\n",
    "print(f\"Vectorizer saved to {vectorizer_filename}\")\n",
    "\n",
    "# Save the logistic regression model\n",
    "model_filename = 'logistic_regression_model.pkl'\n",
    "joblib.dump(logistic_model, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# Save the label encoder\n",
    "label_encoder_filename = 'label_encoder.pkl'\n",
    "joblib.dump(le, label_encoder_filename)\n",
    "print(f\"Label encoder saved to {label_encoder_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logistic_model.pkl', 'wb') as file:\n",
    "    logistic_model = pickle.dump(logistic_model,file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    vectorizer = pickle.dump(tfidf_vectorizer,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating                                             Review sentiment\n",
      "0     4.0  Recently I travelled for business from Cape To...  positive\n",
      "1     5.0  I want to thanks to Jane for her excellent ser...  positive\n",
      "2     4.0  not a bad airline to fly with could do with a ...  positive\n",
      "3     1.0  Dear Kenya Airways ,\\r\\n\\r\\nI am on my way to ...  negative\n",
      "4     1.0  Absolutely appalling airline, never ever use i...  negative\n",
      "Model: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.93      0.85      0.89       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.93      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9220308250226654\n",
      "------------------------------------------------------------\n",
      "Model: RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.93      0.83      0.88       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9165911151405258\n",
      "------------------------------------------------------------\n",
      "Model: XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       346\n",
      "           1       1.00      0.99      1.00       359\n",
      "           2       0.93      0.83      0.88       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9165911151405258\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize NLTK tools\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [ps.stem(w) for w in words if w not in stop_words and w.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_review_sentiment(review):\n",
    "    sentiment_score = sia.polarity_scores(review)['compound']\n",
    "    if sentiment_score > 0.1:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def determine_final_sentiment(row):\n",
    "    review_sentiment = get_review_sentiment(row['Review'])\n",
    "    rating = row['Rating']\n",
    "    \n",
    "    if review_sentiment == 'positive' or rating >= 4:\n",
    "        return 'positive'\n",
    "    elif review_sentiment == 'negative' or rating <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply the function to determine sentiment\n",
    "df['sentiment'] = df.apply(determine_final_sentiment, axis=1)\n",
    "\n",
    "# Check the transformation\n",
    "print(df[['Rating', 'Review', 'sentiment']].head())\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_reviews = df['Review']\n",
    "X_ratings = df['Rating']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Apply the preprocessing to the reviews\n",
    "X_reviews = X_reviews.apply(preprocess_text)\n",
    "\n",
    "# Initialize the vectorizer and fit/transform the text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_reviews_transformed = vectorizer.fit_transform(X_reviews).toarray()\n",
    "\n",
    "# Convert ratings into a 2D array with one feature (reshape for concatenation)\n",
    "X_ratings_transformed = X_ratings.values.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the review features with the rating feature\n",
    "X_combined = np.hstack((X_reviews_transformed, X_ratings_transformed))\n",
    "\n",
    "# Handle missing values by imputing (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_combined = imputer.fit_transform(X_combined)\n",
    "\n",
    "# Encode the target variable (sentiment)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Initialize SMOTE and apply it to the data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to train and evaluate models\n",
    "def train_and_evaluate_model(df, model, text_column='Review', rating_column='Rating', target_column='sentiment'):\n",
    "    \"\"\"\n",
    "    Function to preprocess text and numeric data, train a specified model, and evaluate its performance.\n",
    "    \"\"\"\n",
    "    # Apply preprocessing and transformation steps as described above\n",
    "    # Preprocessing and feature extraction should be done here\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Instantiate models\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Train and evaluate each model\n",
    "train_and_evaluate_model(df, log_reg)\n",
    "train_and_evaluate_model(df, rf)\n",
    "train_and_evaluate_model(df, xgb_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.94      0.85      0.89       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.93      0.93      0.92      1103\n",
      "weighted avg       0.93      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9238440616500453\n",
      "------------------------------------------------------------\n",
      "Model: RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.94      0.84      0.88       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9202175883952856\n",
      "------------------------------------------------------------\n",
      "Model: GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.94      0.83      0.88       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9174977334542158\n",
      "------------------------------------------------------------\n",
      "Model: XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.93      0.84      0.89       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.9211242067089755\n",
      "------------------------------------------------------------\n",
      "Model: SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.88       346\n",
      "           1       1.00      1.00      1.00       359\n",
      "           2       0.95      0.81      0.87       398\n",
      "\n",
      "    accuracy                           0.92      1103\n",
      "   macro avg       0.92      0.92      0.92      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "Accuracy: 0.915684496826836\n",
      "------------------------------------------------------------\n",
      "Model: KNeighborsClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       346\n",
      "           1       0.73      1.00      0.84       359\n",
      "           2       1.00      0.60      0.75       398\n",
      "\n",
      "    accuracy                           0.82      1103\n",
      "   macro avg       0.85      0.83      0.81      1103\n",
      "weighted avg       0.85      0.82      0.81      1103\n",
      "\n",
      "Accuracy: 0.8168631006346329\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize NLTK tools\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [ps.stem(w) for w in words if w not in stop_words and w.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_review_sentiment(review):\n",
    "    sentiment_score = sia.polarity_scores(review)['compound']\n",
    "    if sentiment_score > 0.1:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def determine_final_sentiment(row):\n",
    "    review_sentiment = get_review_sentiment(row['Review'])\n",
    "    rating = row['Rating']\n",
    "    \n",
    "    if review_sentiment == 'positive' or rating >= 4:\n",
    "        return 'positive'\n",
    "    elif review_sentiment == 'negative' or rating <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Load your dataset\n",
    "data = df\n",
    "\n",
    "# Apply the function to determine sentiment\n",
    "data['sentiment'] = data.apply(determine_final_sentiment, axis=1)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_reviews = data['Review']\n",
    "X_ratings = data['Rating']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Apply the preprocessing to the reviews\n",
    "X_reviews = X_reviews.apply(preprocess_text)\n",
    "\n",
    "# Initialize the vectorizer and fit/transform the text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_reviews_transformed = vectorizer.fit_transform(X_reviews).toarray()\n",
    "\n",
    "# Convert ratings into a 2D array with one feature (reshape for concatenation)\n",
    "X_ratings_transformed = X_ratings.values.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the review features with the rating feature\n",
    "X_combined = np.hstack((X_reviews_transformed, X_ratings_transformed))\n",
    "\n",
    "# Handle missing values by imputing (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_combined = imputer.fit_transform(X_combined)\n",
    "\n",
    "# Encode the target variable (sentiment)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Initialize SMOTE and apply it to the data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model. Print classification metrics for classification models\n",
    "    and RMSE for regression models.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):  # Check if the model has predict_proba method\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "        print(\"-\" * 60)\n",
    "    else:\n",
    "        # If model does not have predict_proba, assume it's a regression model\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print('RMSE:', rmse)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Define a list of models to evaluate\n",
    "models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    SVC(probability=True),\n",
    "    KNeighborsClassifier(n_neighbors=5)\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model in models:\n",
    "    train_and_evaluate_model(model, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Reviews:\n",
      "                                               Review\n",
      "3   Dear Kenya Airways ,\\r\\n\\r\\nI am on my way to ...\n",
      "7   After reading the reviews on here, I was prepa...\n",
      "8   Kenya Airways is very disorganzied, first of a...\n",
      "13  My Thai husband and I were planning on traveli...\n",
      "14  My wife and myself flew from Nairobi to Zanzib...\n",
      "Positive Reviews:\n",
      "                                               Review\n",
      "0   Recently I travelled for business from Cape To...\n",
      "2   not a bad airline to fly with could do with a ...\n",
      "5   We flew from Heathrow to Nairobi on our way to...\n",
      "11  Great service and friendly staff most of all B...\n",
      "15  Pleasantly surprised with this airline. Servic...\n",
      "Neutral Reviews:\n",
      "                                                 Review\n",
      "1179  Food was not eatable. Media was not subtitled ...\n",
      "1233  The Dreamliner flight from Nairobi to Amsterda...\n",
      "1248  The flight was on time as per schedule but the...\n",
      "2758  Heathrow to Harare return. Outbound flight was...\n",
      "2841  Missed connection in Nairobi was… KGL to BKK v...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize NLTK tools\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [ps.stem(w) for w in words if w not in stop_words and w.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_review_sentiment(review):\n",
    "    sentiment_score = sia.polarity_scores(review)['compound']\n",
    "    if sentiment_score > 0.1:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Load your dataset\n",
    "data = df\n",
    "\n",
    "# Apply the function to determine sentiment\n",
    "data['sentiment'] = data['Review'].apply(get_review_sentiment)\n",
    "\n",
    "# Preprocess text for further analysis\n",
    "data['Review_processed'] = data['Review'].apply(preprocess_text)\n",
    "\n",
    "# Function to extract and display reviews with specific sentiment and keywords\n",
    "def analyze_sentiment_reviews(data, sentiment_label, keywords=None):\n",
    "    filtered_data = data[data['sentiment'] == sentiment_label]\n",
    "    \n",
    "    if keywords:\n",
    "        keyword_reviews = filtered_data[filtered_data['Review_processed'].str.contains('|'.join(keywords))]\n",
    "    else:\n",
    "        keyword_reviews = filtered_data\n",
    "\n",
    "    return keyword_reviews\n",
    "\n",
    "# Example: Analyze reviews with negative sentiment and specific keywords\n",
    "negative_keywords = ['bad', 'poor', 'terrible', 'worst', 'disappointing']\n",
    "positive_keywords = ['good', 'excellent', 'fantastic', 'great', 'wonderful']\n",
    "neutral_keywords = ['okay', 'average', 'fair', 'normal', 'typical']\n",
    "\n",
    "# Extract and print reviews with negative sentiment and keywords\n",
    "negative_reviews = analyze_sentiment_reviews(data, 'negative', negative_keywords)\n",
    "print(\"Negative Reviews:\")\n",
    "print(negative_reviews[['Review']].head())\n",
    "\n",
    "# Extract and print reviews with positive sentiment and keywords\n",
    "positive_reviews = analyze_sentiment_reviews(data, 'positive', positive_keywords)\n",
    "print(\"Positive Reviews:\")\n",
    "print(positive_reviews[['Review']].head())\n",
    "\n",
    "# Extract and print reviews with neutral sentiment and keywords\n",
    "neutral_reviews = analyze_sentiment_reviews(data, 'neutral', neutral_keywords)\n",
    "print(\"Neutral Reviews:\")\n",
    "print(neutral_reviews[['Review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
